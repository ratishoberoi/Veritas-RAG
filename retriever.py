import os
import json
import re
from typing import Optional, List
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.messages import HumanMessage
from langchain_core.documents import Document
from vector_store import load_vector_store, build_hybrid_retriever
from reranker import rerank_documents, rerank_with_explanation

load_dotenv()

SYSTEM_PROMPT = """You are an intelligent Personal Knowledge Base Assistant.
Your job is to answer questions strictly based on the context retrieved from the user's documents.

Rules:
- Answer ONLY using the provided context below.
- If the answer is not in the context, say exactly: "I couldn't find this in your knowledge base."
- Never hallucinate or make up facts.
- Be concise and direct.
- If the answer comes from a specific source, mention it naturally (e.g. "According to the PDF..." or "In the YouTube video...").
- Use the chat history to understand follow-up questions and pronouns like "it", "they", "why", "explain more".
- NEVER use outside knowledge. If you do, you MUST prefix that sentence with [OUTSIDE KNOWLEDGE].

Context:
{context}
"""

EVALUATOR_PROMPT = """You are a strict RAG evaluation judge.

You will be given:
1. A QUESTION asked by the user
2. CONTEXT retrieved from a vector database
3. An ANSWER generated by an AI assistant

Your job is to evaluate the answer and return a JSON object with exactly these fields:
{{
  "context_sufficient": true or false,
  "confidence_score": integer between 0 and 100,
  "hallucination_detected": true only if the answer contains claims NOT in the context AND NOT an "I don't know" response,
  "hallucination_reason": "short explanation or empty string",
  "evaluation_summary": "one sentence summary"
}}

Scoring guide for confidence_score:
- 90-100: Answer is fully supported by context, every claim traceable
- 70-89:  Answer is mostly supported, minor gaps
- 50-69:  Answer is partially supported, some claims unverifiable
- 20-49:  Answer is weakly supported, mostly guessing
- 0-19:   Answer has no support in context, likely hallucinated

QUESTION: {question}

CONTEXT: {context}

ANSWER: {answer}

Return ONLY the JSON object. No explanation, no markdown, no code blocks."""

store = {}
_session_chunks: List[Document] = []


def register_chunks(chunks: List[Document]):
    """Register ingested chunks into BM25 corpus cache."""
    global _session_chunks
    _session_chunks.extend(chunks)
    seen = set()
    deduped = []
    for doc in _session_chunks:
        if doc.page_content not in seen:
            seen.add(doc.page_content)
            deduped.append(doc)
    _session_chunks = deduped
    print(f"[Retriever] ğŸ“š BM25 corpus: {len(_session_chunks)} unique chunks registered.")


def get_session_chunks() -> List[Document]:
    return _session_chunks


def get_llm(model: str = "llama-3.1-8b-instant") -> ChatGroq:
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        raise ValueError("[Retriever] âŒ GROQ_API_KEY not found in .env")
    return ChatGroq(api_key=api_key, model_name=model, temperature=0)


def format_docs(docs: list) -> str:
    formatted = []
    for i, doc in enumerate(docs):
        source = doc.metadata.get("source", "unknown")
        method = doc.metadata.get("method", "unknown")
        page = doc.metadata.get("page", "")
        title = doc.metadata.get("title", "")
        rerank_score = doc.metadata.get("rerank_score", None)

        source_label = f"Source {i+1}"
        if "youtube" in str(source).lower() or method == "groq_whisper":
            source_label += f" [YouTube: {title}]"
        elif str(source).endswith(".pdf"):
            source_label += f" [PDF: {os.path.basename(str(source))}"
            if page != "":
                source_label += f", Page {int(float(page))+1}"
            source_label += "]"
        if rerank_score is not None:
            source_label += f" (relevance: {rerank_score:.2f})"

        formatted.append(f"{source_label}:\n{doc.page_content}")
    return "\n\n---\n\n".join(formatted)


def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    history = store[session_id]
    if len(history.messages) > 20:
        history.messages = history.messages[-20:]
    return history


def evaluate_response(question: str, context: str, answer: str) -> dict:
    try:
        llm = get_llm()
        eval_prompt = ChatPromptTemplate.from_template(EVALUATOR_PROMPT)
        eval_chain = eval_prompt | llm | StrOutputParser()
        raw = eval_chain.invoke({"question": question, "context": context, "answer": answer})
        raw = re.sub(r"```json|```", "", raw.strip()).strip()
        result = json.loads(raw)
        return {
            "context_sufficient": bool(result.get("context_sufficient", False)),
            "confidence_score": int(result.get("confidence_score", 0)),
            "hallucination_detected": bool(result.get("hallucination_detected", False)),
            "hallucination_reason": result.get("hallucination_reason", ""),
            "evaluation_summary": result.get("evaluation_summary", "")
        }
    except Exception as e:
        print(f"[Evaluator] âš ï¸ Evaluation failed: {e}")
        return {
            "context_sufficient": False, "confidence_score": 0,
            "hallucination_detected": False, "hallucination_reason": "",
            "evaluation_summary": "Evaluation unavailable."
        }


def _build_dense_retriever(
    namespace: Optional[str] = None,
    source_filter: Optional[dict] = None,
    k: int = 12
):
    vector_store = load_vector_store(namespace=namespace)
    search_kwargs = {"k": k}
    if source_filter:
        search_kwargs["filter"] = source_filter
        print(f"[Retriever] ğŸ” Source filter: {source_filter}")
    return vector_store.as_retriever(search_type="similarity", search_kwargs=search_kwargs)


def retrieve_and_rerank(
    query: str,
    retriever,
    use_reranking: bool = True,
    retrieval_k: int = 12,
    final_k: int = 4,
    reranker_model: str = "BAAI/bge-reranker-base",
    score_threshold: Optional[float] = None
) -> List[Document]:
    """
    Two-stage retrieval:

    Stage 1 â€” Broad Recall:
        Fetch `retrieval_k` candidates (default 12) via hybrid/dense.
        High recall ensures the correct chunk is always a candidate.

    Stage 2 â€” Precision Filter (Cross-Encoder):
        Cross-encoder sees (query, chunk) as a single input â€” unlike
        bi-encoders which embed independently. This joint scoring
        captures exact term overlap, co-references, and contextual
        relevance that cosine similarity misses.

        Result: top `final_k` (4) high-precision chunks for LLM.

    Noise reduction example:
        Query: "What is Î²â‚‚ for Adam optimizer?"
        Without rerank: chunks about "beta testing" may rank high
                        (similar embeddings to "beta" concept)
        With rerank:    cross-encoder scores joint relevance,
                        "beta testing" drops to score ~-4, optimizer
                        chunk rises to score ~7 â†’ clean context
    """
    candidates = retriever.invoke(query)

    if not use_reranking or len(candidates) <= final_k:
        return candidates[:final_k]

    print(f"[Retriever] ğŸ¯ Reranking {len(candidates)} â†’ top {final_k}...")
    return rerank_documents(
        query=query,
        docs=candidates,
        top_k=final_k,
        model_name=reranker_model,
        score_threshold=score_threshold,
        return_scores=True
    )


def build_rag_chain(
    namespace: Optional[str] = None,
    source_filter: Optional[dict] = None,
    use_hybrid: bool = True,
    use_reranking: bool = True,
    retrieval_k: int = 12,
    final_k: int = 4,
    bm25_weight: float = 0.4,
    vector_weight: float = 0.6,
    reranker_model: str = "BAAI/bge-reranker-base"
):
    """
    3-stage RAG pipeline:
        1. Hybrid Retrieval (BM25 + Pinecone, RRF fusion) â†’ 12 candidates
        2. Cross-Encoder Reranking (bge-reranker-base) â†’ top 4
        3. LLM Generation (Groq/Llama 3) with conversation memory
    """
    chunks = get_session_chunks()

    if use_hybrid and chunks:
        try:
            base_retriever = build_hybrid_retriever(
                chunks=chunks,
                namespace=namespace,
                source_filter=source_filter,
                k=retrieval_k,
                bm25_weight=bm25_weight,
                vector_weight=vector_weight
            )
            retriever_label = "hybrid"
        except Exception as e:
            print(f"[Retriever] âš ï¸ Hybrid failed ({e}), using dense.")
            base_retriever = _build_dense_retriever(namespace, source_filter, k=retrieval_k)
            retriever_label = "dense"
    else:
        if use_hybrid and not chunks:
            print("[Retriever] âš ï¸ No BM25 corpus â€” dense-only retrieval.")
        base_retriever = _build_dense_retriever(namespace, source_filter, k=retrieval_k)
        retriever_label = "dense"

    rerank_label = f"+rerank({reranker_model.split('/')[-1]})" if use_reranking else ""
    print(f"[Retriever] ğŸ” Pipeline: {retriever_label}{rerank_label} â†’ top{final_k}")

    llm = get_llm()

    prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}")
    ])

    def extract_question(x):
        if isinstance(x, dict):
            return x.get("question", "")
        return str(x)

    def retrieval_pipeline(x):
        q = extract_question(x)
        return retrieve_and_rerank(
            query=q,
            retriever=base_retriever,
            use_reranking=use_reranking,
            retrieval_k=retrieval_k,
            final_k=final_k,
            reranker_model=reranker_model
        )

    core_chain = (
        {
            "context": RunnableLambda(retrieval_pipeline) | format_docs,
            "question": RunnablePassthrough() | (lambda x: extract_question(x)),
            "chat_history": lambda x: x.get("chat_history", []) if isinstance(x, dict) else []
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    chain_with_history = RunnableWithMessageHistory(
        core_chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history"
    )

    print("[Retriever] âœ… RAG chain ready.")
    return chain_with_history, base_retriever


def ask(
    question: str,
    session_id: str = "default",
    namespace: Optional[str] = None,
    source_filter: Optional[dict] = None,
    use_hybrid: bool = True,
    use_reranking: bool = True
) -> dict:
    chain, base_retriever = build_rag_chain(
        namespace=namespace,
        source_filter=source_filter,
        use_hybrid=use_hybrid,
        use_reranking=use_reranking
    )

    final_docs = retrieve_and_rerank(
        query=question,
        retriever=base_retriever,
        use_reranking=use_reranking,
        retrieval_k=12,
        final_k=4
    )
    context_str = format_docs(final_docs)

    answer = chain.invoke(
        {"question": question},
        config={"configurable": {"session_id": session_id}}
    )

    evaluation = evaluate_response(question=question, context=context_str, answer=answer)

    seen = set()
    sources_used = []
    for doc in final_docs:
        source = doc.metadata.get("source", "unknown")
        method = doc.metadata.get("method", "unknown")
        title = doc.metadata.get("title", "")
        page = doc.metadata.get("page", "")
        if source not in seen:
            seen.add(source)
            if method == "groq_whisper" or "youtube" in str(source).lower():
                sources_used.append(f"ğŸ¥ YouTube: {title}")
            else:
                page_info = f", Page {int(float(page))+1}" if page != "" else ""
                sources_used.append(f"ğŸ“„ PDF: {os.path.basename(str(source))}{page_info}")

    chunks = get_session_chunks()
    parts = ["ğŸ”€ Hybrid" if chunks else "ğŸ”· Dense"]
    if use_reranking:
        parts.append("+ ğŸ¯ Reranked")
    retrieval_mode = " ".join(parts)

    return {
        "answer": answer,
        "sources": sources_used,
        "evaluation": evaluation,
        "session_id": session_id,
        "retrieval_mode": retrieval_mode
    }


def render_evaluation(evaluation: dict):
    score = evaluation["confidence_score"]
    sufficient = evaluation["context_sufficient"]
    hallucination = evaluation["hallucination_detected"]
    summary = evaluation["evaluation_summary"]
    reason = evaluation["hallucination_reason"]

    if score >= 70 and sufficient and not hallucination:
        status, bar_color = "âœ… Context Verified", "ğŸŸ¢"
    elif score >= 40:
        status, bar_color = "âš ï¸  Moderate Confidence", "ğŸŸ¡"
    else:
        status, bar_color = "ğŸ”´ Low Confidence", "ğŸ”´"

    bar = bar_color * int(score / 10) + "â¬œ" * (10 - int(score / 10))
    print(f"\nğŸ“Š Evaluation:")
    print(f"   Status     : {status}")
    print(f"   Confidence : {bar} {score}%")
    print(f"   Summary    : {summary}")
    if hallucination:
        print(f"   âš ï¸  Hallucination Flag: {reason}")
    if not sufficient:
        print(f"   â„¹ï¸  Context insufficient for a complete answer.")


if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ§  Neural KB â€” Terminal Chat")
    print("   Hybrid Search + Cross-Encoder Reranking")
    print("="*60)
    print("Commands: exit | clear | filter pdf | filter yt | filter off")
    print("          history | mode | debug <query>")
    print("-"*60 + "\n")

    SESSION_ID = "terminal_session"
    active_filter = None

    try:
        from ingestor import ingest as _ingest
        _pdf = "data/transformer_paper.pdf"
        if os.path.exists(_pdf):
            print(f"[Retriever] ğŸ“„ Pre-loading BM25 corpus from {_pdf}...")
            _chunks = _ingest(source=_pdf, source_type="pdf")
            register_chunks(_chunks)
    except Exception as _e:
        print(f"[Retriever] âš ï¸ BM25 pre-load skipped: {_e}")

    print("[Retriever] ğŸ”„ Initializing (reranker loads on first query)...")
    try:
        chain, retriever = build_rag_chain(source_filter=active_filter)
    except Exception as e:
        print(f"[Retriever] âŒ Failed: {e}")
        exit(1)
    print("[Retriever] âœ… Ready!\n")

    while True:
        try:
            filter_label = f" [{list(active_filter.values())[0]}]" if active_filter else ""
            question = input(f"You{filter_label}: ").strip()
            if not question:
                continue

            if question.lower() in ["exit", "quit", "q"]:
                print("ğŸ‘‹ Goodbye!")
                break

            if question.lower() == "clear":
                if SESSION_ID in store:
                    store[SESSION_ID] = ChatMessageHistory()
                print("ğŸ—‘ï¸  Cleared.\n")
                continue

            if question.lower() == "mode":
                c = get_session_chunks()
                print(f"\nğŸ” {('ğŸ”€ Hybrid BM25+Dense' if c else 'ğŸ”· Dense')} â†’ ğŸ¯ Rerank â†’ top 4 â†’ Llama 3")
                print(f"   BM25 corpus: {len(c)} chunks\n")
                continue

            if question.lower().startswith("debug "):
                dq = question[6:].strip()
                cands = retriever.invoke(dq)
                _, report = rerank_with_explanation(dq, cands, top_k=4)
                print(f"\n{'Rank':<5} {'Score':<8} {'Keep':<6} {'Source':<35} Preview")
                print("-"*90)
                for r in report:
                    print(f"{r['rank']:<5} {r['score']:<8.3f} {'âœ…' if r['kept'] else 'âŒ':<6} "
                          f"{r['source']:<35} {r['preview']}")
                print()
                continue

            if question.lower() == "filter pdf":
                active_filter = {"source": "pdf"}
                chain, retriever = build_rag_chain(source_filter=active_filter)
                print("ğŸ” PDF only.\n")
                continue

            if question.lower() == "filter yt":
                active_filter = {"method": "groq_whisper"}
                chain, retriever = build_rag_chain(source_filter=active_filter)
                print("ğŸ” YouTube only.\n")
                continue

            if question.lower() == "filter off":
                active_filter = None
                chain, retriever = build_rag_chain(source_filter=None)
                print("ğŸ” Filter removed.\n")
                continue

            if question.lower() == "history":
                history = get_session_history(SESSION_ID)
                if not history.messages:
                    print("ğŸ“­ No history yet.\n")
                else:
                    print("\nğŸ“œ Chat History:")
                    for msg in history.messages:
                        print(f"  {'You' if isinstance(msg, HumanMessage) else 'ğŸ¤–'}: {msg.content[:120]}...")
                    print()
                continue

            print("\nğŸ¤” Thinking...\n")

            final_docs = retrieve_and_rerank(
                query=question, retriever=retriever,
                use_reranking=True, retrieval_k=12, final_k=4
            )
            context_str = format_docs(final_docs)

            answer = chain.invoke(
                {"question": question},
                config={"configurable": {"session_id": SESSION_ID}}
            )

            print(f"ğŸ¤– Assistant: {answer}")

            history = get_session_history(SESSION_ID)
            turn_count = len([m for m in history.messages if isinstance(m, HumanMessage)])
            c = get_session_chunks()
            print(f"\nğŸ’¬ Memory: {turn_count} | {'ğŸ”€ Hybrid + ğŸ¯ Reranked' if c else 'ğŸ”· Dense + ğŸ¯ Reranked'}")

            seen = set()
            print("ğŸ“š Sources (reranked):")
            for doc in final_docs:
                src = doc.metadata.get("source", "unknown")
                method = doc.metadata.get("method", "")
                title = doc.metadata.get("title", "")
                page = doc.metadata.get("page", "")
                score = doc.metadata.get("rerank_score")
                score_str = f" [score:{score:.2f}]" if score is not None else ""
                if src not in seen:
                    seen.add(src)
                    if method == "groq_whisper":
                        print(f"  ğŸ¥ {title}{score_str}")
                    else:
                        pg = f", p{int(float(page))+1}" if page != "" else ""
                        print(f"  ğŸ“„ {os.path.basename(str(src))}{pg}{score_str}")

            print("\nâ³ Evaluating...")
            evaluation = evaluate_response(question, context_str, answer)
            render_evaluation(evaluation)
            print("\n" + "-"*60 + "\n")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"[Retriever] âŒ Error: {e}\n")
            continue